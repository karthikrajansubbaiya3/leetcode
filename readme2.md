# tokenization :break down words into token it can bit sized recognize 
# attention : attention is how ollama decides which parts of text matter most for understanding
 # identify -the model spots key words and pattern in text
 # connect -it link related concepts across sentence and paragraph
 # process - the model perform a cohesive understanding of meaning
 # it looks at all the token it seen so far and decides which ones are important to pay attention
 # is called self attention mechanism
 # limited space models can focus finite number of tokens at once
 # memory limits like a white board,older information must be  erased to make roo for new room
 # scaling up larger model have a bigger white board and can handel longer text
 # traning data 
 # the data diet llama 4 learned from massive amounts of internet text,books and conversation
 # it doesnot share this content instead it extract pattern and relationship
 # 1 pattern recognition:learn how words relate to each other in various contexts
 # 2 language flow :understand how sentence naturally progress and connect
 # 3 the cooking analogy like reading thousand of cookbooks and learnig  to cook without memorising recipies
 # llama-1 2048 tokens llama-2 -4096 tokens
 # llama3-128000 tokens
 # what is an llm
 # AI trained on vast text data to learn language pattern.takes prompts and predicts likely continuation
 # parameter models internal setting like ai synapes "more parameter often mean a bigger brain for processing
 # traning data-llms learn form tokenized text and images llama model train on billion of tokens diverse source
 # capabilites - answer question,converse,write content,translate language summarize and assit with coding
